\contentsline {section}{\numberline {1}Introduction }{4}{section.1}%
\contentsline {subsection}{\numberline {1.1}Overview}{4}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Elements of Reinforcement Learning}{4}{subsection.1.2}%
\contentsline {section}{\numberline {2}Multi-arm Bandits}{5}{section.2}%
\contentsline {subsection}{\numberline {2.1}An n-Armed Bandit Problem}{5}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Action-Value Methods}{5}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Incremental Implementation}{6}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Tracking a Nonstationary Problem}{7}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Optimistic Initial Values}{7}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}Upper-confidence-bound Action Selection}{8}{subsection.2.6}%
\contentsline {subsection}{\numberline {2.7}Associative Search (Contextual Bandits)}{8}{subsection.2.7}%
\contentsline {subsection}{\numberline {2.8}Key Takeaways}{9}{subsection.2.8}%
\contentsline {section}{\numberline {3}Finite Markov Decision Processes}{10}{section.3}%
\contentsline {subsection}{\numberline {3.1}The Agent-Environment Interface}{10}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Goals and Rewards}{10}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Returns and Episodes}{10}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Unified Notation for Episodic and Continuing Tasks}{11}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}The Markov Property}{11}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Markov Decision Process (MDP)}{12}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Policies and Value Functions}{12}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Optimal Policies and Value Functions}{13}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Optimality and Approximation}{14}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Key Takeaways}{14}{subsection.3.10}%
\contentsline {section}{\numberline {4}Dynamic Programming}{15}{section.4}%
\contentsline {subsection}{\numberline {4.1}Policy Evaluation (Prediction)}{15}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Policy Improvement}{15}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Policy Iteration}{16}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Value Iteration}{16}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Asynchronous Dynamic Programming}{16}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Generalised Policy Iteration}{16}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}Key Takeaways}{17}{subsection.4.7}%
\contentsline {section}{\numberline {5}Monte Carlo Methods}{18}{section.5}%
\contentsline {subsection}{\numberline {5.1}Monte Carlo Policy Prediction}{18}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Monte Carlo Estimation of Action Values}{19}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Monte Carlo Control}{19}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Monte Carlo Control without Exploring Starts}{19}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Off-policy Prediction via Importance Sampling}{20}{subsection.5.5}%
\contentsline {subsection}{\numberline {5.6}Incremental Implementation}{21}{subsection.5.6}%
\contentsline {subsection}{\numberline {5.7}Off-policy Monte Carlo Control}{21}{subsection.5.7}%
\contentsline {subsection}{\numberline {5.8}Key Takeaways}{21}{subsection.5.8}%
\contentsline {section}{\numberline {6}Temporal-Difference Learning}{24}{section.6}%
\contentsline {subsection}{\numberline {6.1}TD Prediction}{24}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Advantages of TD Prediction Methods}{24}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Optimality of TD(0)}{25}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Sarsa: On-policy TD Control}{25}{subsection.6.4}%
\contentsline {subsection}{\numberline {6.5}Q-learning: Off-policy TD Control}{26}{subsection.6.5}%
\contentsline {subsection}{\numberline {6.6}Expected Sarsa}{26}{subsection.6.6}%
\contentsline {subsection}{\numberline {6.7}Maximization Bias and Double Learning}{26}{subsection.6.7}%
\contentsline {subsection}{\numberline {6.8}Games, Afterstates, and Other Special Cases}{26}{subsection.6.8}%
\contentsline {subsection}{\numberline {6.9}Key Takeaways}{27}{subsection.6.9}%
\contentsline {section}{\numberline {7}$n$-step Bootstrapping}{28}{section.7}%
\contentsline {subsection}{\numberline {7.1}$n$-step TD Prediction}{28}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}$n$-step Sarsa}{29}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}$n$-step Off-policy Learning}{29}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Per-decision Methods with Control Variates}{30}{subsection.7.4}%
\contentsline {subsection}{\numberline {7.5}Off-policy Learning Without Importance Sampling: The $n$-step Tree Backup Algorithm}{30}{subsection.7.5}%
\contentsline {subsection}{\numberline {7.6}A Unifying Algorithm: $n$-step Q($\sigma $)}{31}{subsection.7.6}%
\contentsline {subsection}{\numberline {7.7}Key Takeaways}{31}{subsection.7.7}%
\contentsline {section}{\numberline {8}Planning and Learning with Tabular Methods}{33}{section.8}%
\contentsline {subsection}{\numberline {8.1}Models and Planning}{33}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Dyna: Integrated Planning, Acting, and Learning}{33}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}When the Model is Wrong}{34}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Prioritized Sweeping}{35}{subsection.8.4}%
\contentsline {subsection}{\numberline {8.5}Expected vs. Sample Updates}{35}{subsection.8.5}%
\contentsline {subsection}{\numberline {8.6}Trajectory Sampling}{36}{subsection.8.6}%
\contentsline {subsection}{\numberline {8.7}Real-time Dynamic Programming}{36}{subsection.8.7}%
\contentsline {subsection}{\numberline {8.8}Planning at Decision Time}{37}{subsection.8.8}%
\contentsline {subsection}{\numberline {8.9}Heuristic Search}{37}{subsection.8.9}%
\contentsline {subsection}{\numberline {8.10}Rollout Algorithms}{37}{subsection.8.10}%
\contentsline {subsection}{\numberline {8.11}Monte Carlo Tree Search}{38}{subsection.8.11}%
\contentsline {subsection}{\numberline {8.12}Key Takeaways}{38}{subsection.8.12}%
\contentsline {subsection}{\numberline {8.13}Summary of Part 1}{40}{subsection.8.13}%
\contentsline {section}{\numberline {9}On-policy Prediction with Approximation}{42}{section.9}%
\contentsline {subsection}{\numberline {9.1}Value-function Approximation}{42}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}The Prediction Objective ($\bar {VE}$)}{42}{subsection.9.2}%
\contentsline {subsection}{\numberline {9.3}Stochastic-gradient and Semi-gradient Methods}{42}{subsection.9.3}%
\contentsline {subsection}{\numberline {9.4}Linear Methods}{43}{subsection.9.4}%
\contentsline {subsection}{\numberline {9.5}Feature Construction for Linear Models}{43}{subsection.9.5}%
\contentsline {subsubsection}{\numberline {9.5.1}Polynomials}{43}{subsubsection.9.5.1}%
\contentsline {subsubsection}{\numberline {9.5.2}Fourier Basis}{44}{subsubsection.9.5.2}%
\contentsline {subsubsection}{\numberline {9.5.3}Coarse Coding}{45}{subsubsection.9.5.3}%
\contentsline {subsubsection}{\numberline {9.5.4}Tile Coding}{45}{subsubsection.9.5.4}%
\contentsline {subsubsection}{\numberline {9.5.5}Radial Basis Functions}{45}{subsubsection.9.5.5}%
\contentsline {subsection}{\numberline {9.6}Selecting Step-Size Parameters Manually}{46}{subsection.9.6}%
\contentsline {subsection}{\numberline {9.7}Nonlinear Function Approximation: Artificial Neural Networks}{46}{subsection.9.7}%
\contentsline {subsection}{\numberline {9.8}Least-Squares TD}{47}{subsection.9.8}%
\contentsline {subsection}{\numberline {9.9}Memory-based Function Approximation}{47}{subsection.9.9}%
\contentsline {subsection}{\numberline {9.10}Kernel-based Function Approximation}{48}{subsection.9.10}%
\contentsline {subsection}{\numberline {9.11}Looking Deeper at On-policy Learning: Interest and Emphasis}{48}{subsection.9.11}%
\contentsline {subsection}{\numberline {9.12}Key Takeaways}{48}{subsection.9.12}%
