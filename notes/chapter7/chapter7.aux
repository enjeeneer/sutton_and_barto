\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {7}$n$-step Bootstrapping}{29}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}$n$-step TD Prediction}{29}{subsection.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Backup diagram for TD(0)\relax }}{29}{figure.caption.13}\protected@file@percent }
\newlabel{fig: n-step methods}{{12}{29}{Backup diagram for TD(0)\relax }{figure.caption.13}{}}
\newlabel{key}{{7.1}{29}{$n$-step TD Prediction}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}$n$-step Sarsa}{30}{subsection.7.2}\protected@file@percent }
\newlabel{eq: n-step sarsa}{{45}{30}{$n$-step Sarsa}{equation.7.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Backup diagrams for family of $n$-step Sarsa algorithms\relax }}{30}{figure.caption.14}\protected@file@percent }
\newlabel{fig: n-step sarsa methods}{{13}{30}{Backup diagrams for family of $n$-step Sarsa algorithms\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Gridworld example of the speedup of policy learning due to the use of n-step methods. The first panel shows the path taken by an agent in a single episode, ending at a location of high reward, marked by the G. In this example the values were all initially 0, and all rewards were zero except for a positive reward at G. The arrows in the other two panels show which action values were strengthened as a result of this path by one-step and n-step Sarsa methods. The one-step method strengthens only the last action of the sequence of actions that led to the high reward, whereas the n-step method strengthens the last n actions of the sequence, so that much more is learned from the one episode.\relax }}{30}{figure.caption.15}\protected@file@percent }
\newlabel{fig: n-step sarsa speed up}{{14}{30}{Gridworld example of the speedup of policy learning due to the use of n-step methods. The first panel shows the path taken by an agent in a single episode, ending at a location of high reward, marked by the G. In this example the values were all initially 0, and all rewards were zero except for a positive reward at G. The arrows in the other two panels show which action values were strengthened as a result of this path by one-step and n-step Sarsa methods. The one-step method strengthens only the last action of the sequence of actions that led to the high reward, whereas the n-step method strengthens the last n actions of the sequence, so that much more is learned from the one episode.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}$n$-step Off-policy Learning}{31}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Per-decision Methods with Control Variates}{31}{subsection.7.4}\protected@file@percent }
\newlabel{eq: off policy n step}{{49}{31}{Per-decision Methods with Control Variates}{equation.7.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Off-policy Learning Without Importance Sampling: The $n$-step Tree Backup Algorithm}{31}{subsection.7.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The 3-step tree-backup update\relax }}{32}{figure.caption.16}\protected@file@percent }
\newlabel{fig: tree-backup update}{{15}{32}{The 3-step tree-backup update\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}A Unifying Algorithm: $n$-step Q($\sigma $)}{32}{subsection.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7}Key Takeaways}{32}{subsection.7.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The backup diagrams of the three kinds of n-step action-value updates considered so far in this chapter (4-step case) plus the backup diagram of a fourth kind of update that unifies them all. The label ‘$\rho $’ indicates half transitions on which importance sampling is required in the off-policy case. The fourth kind of update unifies all the others by choosing o a state-by-state basis whether to sample ($\sigma _t$ = 1) or not ($\sigma _t$= 0).\relax }}{33}{figure.caption.17}\protected@file@percent }
\newlabel{fig: all chap 7 backup diagrams}{{16}{33}{The backup diagrams of the three kinds of n-step action-value updates considered so far in this chapter (4-step case) plus the backup diagram of a fourth kind of update that unifies them all. The label ‘$\rho $’ indicates half transitions on which importance sampling is required in the off-policy case. The fourth kind of update unifies all the others by choosing o a state-by-state basis whether to sample ($\sigma _t$ = 1) or not ($\sigma _t$= 0).\relax }{figure.caption.17}{}}
\@setckpt{chapter7/chapter7}{
\setcounter{page}{34}
\setcounter{equation}{52}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{7}
\setcounter{subsection}{7}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{16}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{58}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{section@level}{2}
}
