\contentsline {section}{\numberline {1}Introduction }{3}{section.1}%
\contentsline {subsection}{\numberline {1.1}Overview}{3}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Elements of Reinforcement Learning}{3}{subsection.1.2}%
\contentsline {section}{\numberline {2}Multi-arm Bandits}{4}{section.2}%
\contentsline {subsection}{\numberline {2.1}An n-Armed Bandit Problem}{4}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Action-Value Methods}{4}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Incremental Implementation}{5}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Tracking a Nonstationary Problem}{6}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Optimistic Initial Values}{6}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}Upper-confidence-bound Action Selection}{7}{subsection.2.6}%
\contentsline {subsection}{\numberline {2.7}Associative Search (Contextual Bandits)}{7}{subsection.2.7}%
\contentsline {subsection}{\numberline {2.8}Key Takeaways}{8}{subsection.2.8}%
\contentsline {section}{\numberline {3}Finite Markov Decision Processes}{9}{section.3}%
\contentsline {subsection}{\numberline {3.1}The Agent-Environment Interface}{9}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Goals and Rewards}{9}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Returns and Episodes}{9}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Unified Notation for Episodic and Continuing Tasks}{10}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}The Markov Property}{10}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Markov Decision Process (MDP)}{11}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Policies and Value Functions}{11}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Optimal Policies and Value Functions}{12}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Optimality and Approximation}{13}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Key Takeaways}{13}{subsection.3.10}%
\contentsline {section}{\numberline {4}Dynamic Programming}{14}{section.4}%
\contentsline {subsection}{\numberline {4.1}Policy Evaluation (Prediction)}{14}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Policy Improvement}{14}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Policy Iteration}{15}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Value Iteration}{15}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Asynchronous Dynamic Programming}{15}{subsection.4.5}%
\contentsline {subsection}{\numberline {4.6}Generalised Policy Iteration}{15}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}Key Takeaways}{16}{subsection.4.7}%
\contentsline {section}{\numberline {5}Monte Carlo Methods}{17}{section.5}%
\contentsline {subsection}{\numberline {5.1}Monte Carlo Policy Prediction}{17}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Monte Carlo Estimation of Action Values}{18}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Monte Carlo Control}{18}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Monte Carlo Control without Exploring Starts}{18}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}Off-policy Prediction via Importance Sampling}{19}{subsection.5.5}%
\contentsline {subsection}{\numberline {5.6}Incremental Implementation}{20}{subsection.5.6}%
\contentsline {subsection}{\numberline {5.7}Off-policy Monte Carlo Control}{20}{subsection.5.7}%
\contentsline {subsection}{\numberline {5.8}Key Takeaways}{20}{subsection.5.8}%
\contentsline {section}{\numberline {6}Temporal-Difference Learning}{23}{section.6}%
\contentsline {subsection}{\numberline {6.1}TD Prediction}{23}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Advantages of TD Prediction Methods}{23}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Optimality of TD(0)}{24}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Sarsa: On-policy TD Control}{24}{subsection.6.4}%
\contentsline {subsection}{\numberline {6.5}Q-learning: Off-policy TD Control}{25}{subsection.6.5}%
\contentsline {subsection}{\numberline {6.6}Expected Sarsa}{25}{subsection.6.6}%
\contentsline {subsection}{\numberline {6.7}Maximization Bias and Double Learning}{25}{subsection.6.7}%
\contentsline {subsection}{\numberline {6.8}Games, Afterstates, and Other Special Cases}{25}{subsection.6.8}%
\contentsline {subsection}{\numberline {6.9}Key Takeaways}{26}{subsection.6.9}%
\contentsline {section}{\numberline {7}$n$-step Bootstrapping}{27}{section.7}%
