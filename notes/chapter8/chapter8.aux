\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {8}Planning and Learning with Tabular Methods}{34}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Models and Planning}{34}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Dyna: Integrated Planning, Acting, and Learning}{34}{subsection.8.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The framework for updating our policy using our model of the environment\relax }}{35}{figure.caption.18}\protected@file@percent }
\newlabel{fig: updating policy using model}{{17}{35}{The framework for updating our policy using our model of the environment\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Direct RL and model-learning, either achieved using data collected from experience\relax }}{35}{figure.caption.19}\protected@file@percent }
\newlabel{fig: direct rl and model-learning}{{18}{35}{Direct RL and model-learning, either achieved using data collected from experience\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Generalised Dyna architecture\relax }}{35}{figure.caption.20}\protected@file@percent }
\newlabel{fig: dyna}{{19}{35}{Generalised Dyna architecture\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}When the Model is Wrong}{36}{subsection.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Prioritized Sweeping}{36}{subsection.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Expected vs. Sample Updates}{36}{subsection.8.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Comparison of efficiency of expected and sample updates\relax }}{37}{figure.caption.21}\protected@file@percent }
\newlabel{fig: samples vs expectations}{{20}{37}{Comparison of efficiency of expected and sample updates\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}Trajectory Sampling}{37}{subsection.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.7}Real-time Dynamic Programming}{37}{subsection.8.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8}Planning at Decision Time}{38}{subsection.8.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9}Heuristic Search}{38}{subsection.8.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.10}Rollout Algorithms}{38}{subsection.8.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Heuristic search can be implemented as a sequence of one-step updates (shown here outlined in blue) backing up values from the leaf nodes toward the root. The ordering shown here is for a selective depth-first search.\relax }}{39}{figure.caption.22}\protected@file@percent }
\newlabel{fig: heuristic search}{{21}{39}{Heuristic search can be implemented as a sequence of one-step updates (shown here outlined in blue) backing up values from the leaf nodes toward the root. The ordering shown here is for a selective depth-first search.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.11}Monte Carlo Tree Search}{39}{subsection.8.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.12}Key Takeaways}{39}{subsection.8.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Monte Carlo Tree Search. When the environment changes to a new state, MCTS executes as many iterations as possible before an action needs to be selected, incrementally building a tree whose root node represents the current state. Each iteration consists of the four operations Selection, Expansion (though possibly skipped on some iterations), Simulation, and Backup, as explained in the text and illustrated by the bold arrows in the trees.\relax }}{40}{figure.caption.23}\protected@file@percent }
\newlabel{fig: MCTS}{{22}{40}{Monte Carlo Tree Search. When the environment changes to a new state, MCTS executes as many iterations as possible before an action needs to be selected, incrementally building a tree whose root node represents the current state. Each iteration consists of the four operations Selection, Expansion (though possibly skipped on some iterations), Simulation, and Backup, as explained in the text and illustrated by the bold arrows in the trees.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.13}Summary of Part I}{41}{subsection.8.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces A slice through the space of reinforcement learning methods, highlighting the two of the most important dimensions explored in Part I of this book: the depth and width of the updates.\relax }}{42}{figure.caption.24}\protected@file@percent }
\newlabel{fig: part1 summary}{{23}{42}{A slice through the space of reinforcement learning methods, highlighting the two of the most important dimensions explored in Part I of this book: the depth and width of the updates.\relax }{figure.caption.24}{}}
\@setckpt{chapter8/chapter8}{
\setcounter{page}{43}
\setcounter{equation}{52}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{8}
\setcounter{subsection}{13}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{23}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{6}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{72}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{section@level}{2}
}
