\BOOKMARK [1][-]{section.1}{Introduction }{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Overview}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Elements of Reinforcement Learning}{section.1}% 3
\BOOKMARK [1][-]{section.2}{Multi-arm Bandits}{}% 4
\BOOKMARK [2][-]{subsection.2.1}{An n-Armed Bandit Problem}{section.2}% 5
\BOOKMARK [2][-]{subsection.2.2}{Action-Value Methods}{section.2}% 6
\BOOKMARK [2][-]{subsection.2.3}{Incremental Implementation}{section.2}% 7
\BOOKMARK [2][-]{subsection.2.4}{Tracking a Nonstationary Problem}{section.2}% 8
\BOOKMARK [2][-]{subsection.2.5}{Optimistic Initial Values}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.6}{Upper-confidence-bound Action Selection}{section.2}% 10
\BOOKMARK [2][-]{subsection.2.7}{Associative Search \(Contextual Bandits\)}{section.2}% 11
\BOOKMARK [2][-]{subsection.2.8}{Key Takeaways}{section.2}% 12
\BOOKMARK [1][-]{section.3}{Finite Markov Decision Processes}{}% 13
\BOOKMARK [2][-]{subsection.3.1}{The Agent-Environment Interface}{section.3}% 14
\BOOKMARK [2][-]{subsection.3.2}{Goals and Rewards}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.3}{Returns and Episodes}{section.3}% 16
\BOOKMARK [2][-]{subsection.3.4}{Unified Notation for Episodic and Continuing Tasks}{section.3}% 17
\BOOKMARK [2][-]{subsection.3.5}{The Markov Property}{section.3}% 18
\BOOKMARK [2][-]{subsection.3.6}{Markov Decision Process \(MDP\)}{section.3}% 19
\BOOKMARK [2][-]{subsection.3.7}{Policies and Value Functions}{section.3}% 20
\BOOKMARK [2][-]{subsection.3.8}{Optimal Policies and Value Functions}{section.3}% 21
\BOOKMARK [2][-]{subsection.3.9}{Optimality and Approximation}{section.3}% 22
\BOOKMARK [2][-]{subsection.3.10}{Key Takeaways}{section.3}% 23
\BOOKMARK [1][-]{section.4}{Dynamic Programming}{}% 24
\BOOKMARK [2][-]{subsection.4.1}{Policy Evaluation \(Prediction\)}{section.4}% 25
\BOOKMARK [2][-]{subsection.4.2}{Policy Improvement}{section.4}% 26
\BOOKMARK [2][-]{subsection.4.3}{Policy Iteration}{section.4}% 27
\BOOKMARK [2][-]{subsection.4.4}{Value Iteration}{section.4}% 28
\BOOKMARK [2][-]{subsection.4.5}{Asynchronous Dynamic Programming}{section.4}% 29
\BOOKMARK [2][-]{subsection.4.6}{Generalised Policy Iteration}{section.4}% 30
\BOOKMARK [2][-]{subsection.4.7}{Key Takeaways}{section.4}% 31
\BOOKMARK [1][-]{section.5}{Monte Carlo Methods}{}% 32
\BOOKMARK [2][-]{subsection.5.1}{Monte Carlo Policy Prediction}{section.5}% 33
\BOOKMARK [2][-]{subsection.5.2}{Monte Carlo Estimation of Action Values}{section.5}% 34
\BOOKMARK [2][-]{subsection.5.3}{Monte Carlo Control}{section.5}% 35
\BOOKMARK [2][-]{subsection.5.4}{Monte Carlo Control without Exploring Starts}{section.5}% 36
\BOOKMARK [2][-]{subsection.5.5}{Off-policy Prediction via Importance Sampling}{section.5}% 37
\BOOKMARK [2][-]{subsection.5.6}{Incremental Implementation}{section.5}% 38
\BOOKMARK [2][-]{subsection.5.7}{Off-policy Monte Carlo Control}{section.5}% 39
\BOOKMARK [2][-]{subsection.5.8}{Key Takeaways}{section.5}% 40
\BOOKMARK [1][-]{section.6}{Temporal-Difference Learning}{}% 41
\BOOKMARK [2][-]{subsection.6.1}{TD Prediction}{section.6}% 42
\BOOKMARK [2][-]{subsection.6.2}{Advantages of TD Prediction Methods}{section.6}% 43
\BOOKMARK [2][-]{subsection.6.3}{Optimality of TD\(0\)}{section.6}% 44
\BOOKMARK [2][-]{subsection.6.4}{Sarsa: On-policy TD Control}{section.6}% 45
\BOOKMARK [2][-]{subsection.6.5}{Q-learning: Off-policy TD Control}{section.6}% 46
\BOOKMARK [2][-]{subsection.6.6}{Expected Sarsa}{section.6}% 47
\BOOKMARK [2][-]{subsection.6.7}{Maximization Bias and Double Learning}{section.6}% 48
\BOOKMARK [2][-]{subsection.6.8}{Games, Afterstates, and Other Special Cases}{section.6}% 49
\BOOKMARK [2][-]{subsection.6.9}{Key Takeaways}{section.6}% 50
\BOOKMARK [1][-]{section.7}{n-step Bootstrapping}{}% 51
\BOOKMARK [2][-]{subsection.7.1}{n-step TD Prediction}{section.7}% 52
\BOOKMARK [2][-]{subsection.7.2}{n-step Sarsa}{section.7}% 53
\BOOKMARK [2][-]{subsection.7.3}{n-step Off-policy Learning}{section.7}% 54
\BOOKMARK [2][-]{subsection.7.4}{Per-decision Methods with Control Variates}{section.7}% 55
\BOOKMARK [2][-]{subsection.7.5}{Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm}{section.7}% 56
\BOOKMARK [2][-]{subsection.7.6}{A Unifying Algorithm: n-step Q\(\)}{section.7}% 57
\BOOKMARK [2][-]{subsection.7.7}{Key Takeaways}{section.7}% 58
\BOOKMARK [1][-]{section.8}{Planning and Learning with Tabular Methods}{}% 59
\BOOKMARK [2][-]{subsection.8.1}{Models and Planning}{section.8}% 60
\BOOKMARK [2][-]{subsection.8.2}{Dyna: Integrated Planning, Acting, and Learning}{section.8}% 61
