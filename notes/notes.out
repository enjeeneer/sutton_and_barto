\BOOKMARK [1][-]{section.1}{Introduction }{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Overview}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Elements of Reinforcement Learning}{section.1}% 3
\BOOKMARK [1][-]{section.2}{Multi-arm Bandits}{}% 4
\BOOKMARK [2][-]{subsection.2.1}{An n-Armed Bandit Problem}{section.2}% 5
\BOOKMARK [2][-]{subsection.2.2}{Action-Value Methods}{section.2}% 6
\BOOKMARK [2][-]{subsection.2.3}{Incremental Implementation}{section.2}% 7
\BOOKMARK [2][-]{subsection.2.4}{Tracking a Nonstationary Problem}{section.2}% 8
\BOOKMARK [2][-]{subsection.2.5}{Optimistic Initial Values}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.6}{Upper-confidence-bound Action Selection}{section.2}% 10
\BOOKMARK [2][-]{subsection.2.7}{Associative Search \(Contextual Bandits\)}{section.2}% 11
\BOOKMARK [2][-]{subsection.2.8}{Key Takeaways}{section.2}% 12
\BOOKMARK [1][-]{section.3}{Finite Markov Decision Processes}{}% 13
\BOOKMARK [2][-]{subsection.3.1}{The Agent-Environment Interface}{section.3}% 14
\BOOKMARK [2][-]{subsection.3.2}{Goals and Rewards}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.3}{Returns and Episodes}{section.3}% 16
\BOOKMARK [2][-]{subsection.3.4}{Unified Notation for Episodic and Continuing Tasks}{section.3}% 17
\BOOKMARK [2][-]{subsection.3.5}{The Markov Property}{section.3}% 18
\BOOKMARK [2][-]{subsection.3.6}{Markov Decision Process \(MDP\)}{section.3}% 19
\BOOKMARK [2][-]{subsection.3.7}{Policies and Value Functions}{section.3}% 20
\BOOKMARK [2][-]{subsection.3.8}{Optimal Policies and Value Functions}{section.3}% 21
\BOOKMARK [2][-]{subsection.3.9}{Optimality and Approximation}{section.3}% 22
\BOOKMARK [2][-]{subsection.3.10}{Key Takeaways}{section.3}% 23
\BOOKMARK [1][-]{section.4}{Dynamic Programming}{}% 24
\BOOKMARK [2][-]{subsection.4.1}{Policy Evaluation \(Prediction\)}{section.4}% 25
\BOOKMARK [2][-]{subsection.4.2}{Policy Improvement}{section.4}% 26
\BOOKMARK [2][-]{subsection.4.3}{Policy Iteration}{section.4}% 27
\BOOKMARK [2][-]{subsection.4.4}{Value Iteration}{section.4}% 28
\BOOKMARK [2][-]{subsection.4.5}{Asynchronous Dynamic Programming}{section.4}% 29
\BOOKMARK [2][-]{subsection.4.6}{Generalised Policy Iteration}{section.4}% 30
\BOOKMARK [2][-]{subsection.4.7}{Key Takeaways}{section.4}% 31
\BOOKMARK [1][-]{section.5}{Monte Carlo Methods}{}% 32
\BOOKMARK [2][-]{subsection.5.1}{Monte Carlo Policy Prediction}{section.5}% 33
\BOOKMARK [2][-]{subsection.5.2}{Monte Carlo Estimation of Action Values}{section.5}% 34
\BOOKMARK [2][-]{subsection.5.3}{Monte Carlo Control}{section.5}% 35
\BOOKMARK [2][-]{subsection.5.4}{Monte Carlo Control without Exploring Starts}{section.5}% 36
\BOOKMARK [2][-]{subsection.5.5}{Off-policy Prediction via Importance Sampling}{section.5}% 37
\BOOKMARK [2][-]{subsection.5.6}{Incremental Implementation}{section.5}% 38
\BOOKMARK [2][-]{subsection.5.7}{Off-policy Monte Carlo Control}{section.5}% 39
\BOOKMARK [2][-]{subsection.5.8}{Key Takeaways}{section.5}% 40
\BOOKMARK [1][-]{section.6}{Temporal-Difference Learning}{}% 41
\BOOKMARK [2][-]{subsection.6.1}{TD Prediction}{section.6}% 42
