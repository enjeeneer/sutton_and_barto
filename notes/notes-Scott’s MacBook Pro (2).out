\BOOKMARK [1][-]{section.1}{Introduction }{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Overview}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Elements of Reinforcement Learning}{section.1}% 3
\BOOKMARK [1][-]{section.2}{Multi-arm Bandits}{}% 4
\BOOKMARK [2][-]{subsection.2.1}{An n-Armed Bandit Problem}{section.2}% 5
\BOOKMARK [2][-]{subsection.2.2}{Action-Value Methods}{section.2}% 6
\BOOKMARK [2][-]{subsection.2.3}{Incremental Implementation}{section.2}% 7
\BOOKMARK [2][-]{subsection.2.4}{Tracking a Nonstationary Problem}{section.2}% 8
\BOOKMARK [2][-]{subsection.2.5}{Optimistic Initial Values}{section.2}% 9
\BOOKMARK [2][-]{subsection.2.6}{Upper-confidence-bound Action Selection}{section.2}% 10
\BOOKMARK [2][-]{subsection.2.7}{Associative Search \(Contextual Bandits\)}{section.2}% 11
\BOOKMARK [2][-]{subsection.2.8}{Key Takeaways}{section.2}% 12
\BOOKMARK [1][-]{section.3}{Finite Markov Decision Processes}{}% 13
\BOOKMARK [2][-]{subsection.3.1}{The Agent-Environment Interface}{section.3}% 14
\BOOKMARK [2][-]{subsection.3.2}{Goals and Rewards}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.3}{Returns and Episodes}{section.3}% 16
\BOOKMARK [2][-]{subsection.3.4}{Unified Notation for Episodic and Continuing Tasks}{section.3}% 17
\BOOKMARK [2][-]{subsection.3.5}{The Markov Property}{section.3}% 18
\BOOKMARK [2][-]{subsection.3.6}{Markov Decision Process \(MDP\)}{section.3}% 19
\BOOKMARK [2][-]{subsection.3.7}{Policies and Value Functions}{section.3}% 20
\BOOKMARK [2][-]{subsection.3.8}{Optimal Policies and Value Functions}{section.3}% 21
\BOOKMARK [2][-]{subsection.3.9}{Optimality and Approximation}{section.3}% 22
\BOOKMARK [2][-]{subsection.3.10}{Key Takeaways}{section.3}% 23
